{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJaFcReOWEDQdw5iDRSyuw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/destj-hue/edmonton-property-analysis/blob/main/YEG_Property_Assessments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XanLqdpeYKFU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bokeh.plotting import figure, output_file, show, output_notebook, save\n",
        "from bokeh.models import HoverTool, ColorBar, LinearColorMapper, ColumnDataSource\n",
        "from bokeh.transform import linear_cmap\n",
        "from bokeh.palettes import Viridis256, Category20\n",
        "from bokeh.layouts import column\n",
        "output_notebook()\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_excel('Property_Assessment_Data.xlsx')\n",
        "print(df.head())\n",
        "\n",
        "# Display column names and types\n",
        "print(\"\\nColumn names and data types:\")\n",
        "print(df.dtypes)\n",
        "print(\"\\nColumn names:\")\n",
        "print(df.columns.tolist())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Identify the assessed value column (check actual column name)\n",
        "# Common names: 'assessed_value', 'total_assessed_value', 'Assessed Value'\n",
        "value_col = None\n",
        "for col in df.columns:\n",
        "    if 'assess' in col.lower() and 'value' in col.lower():\n",
        "        value_col = col\n",
        "        print(f\"\\nFound assessed value column: '{value_col}'\")\n",
        "        break\n",
        "\n",
        "if value_col is None:\n",
        "    print(\"\\nPlease identify the assessed value column name:\")\n",
        "    print(df.columns.tolist())\n",
        "    value_col = 'assessed_value'\n",
        "\n",
        "# Convert other numeric columns\n",
        "if 'Latitude' in df.columns:\n",
        "    df['Latitude'] = pd.to_numeric(df['Latitude'], errors='coerce')\n",
        "\n",
        "if 'Longitude' in df.columns:\n",
        "    df['Longitude'] = pd.to_numeric(df['Longitude'], errors='coerce')\n",
        "\n",
        "# Remove outliers (properties with assessed value = 0 or negative)\n",
        "df['assessed_value'] = pd.to_numeric(df['Assessed Value'], errors='coerce')\n",
        "\n",
        "# Remove rows with missing assessed values\n",
        "df = df.dropna(subset=['assessed_value'])\n",
        "df = df[df['assessed_value'] > 0]\n",
        "\n",
        "# Residential Tax Class Filter\n",
        "tax_class_col = None\n",
        "for col in df.columns:\n",
        "    if 'tax' in col.lower() and 'class' in col.lower():\n",
        "        tax_class_col = col\n",
        "        break\n",
        "print(df[tax_class_col].value_counts())\n",
        "df_residential = df[df[tax_class_col] == 'Residential'].copy()\n",
        "df = df_residential\n",
        "print(f\"Residential properties: {len(df_residential)}\")\n",
        "print(f\"Percentage of total: {len(df_residential)/len(df)*100:.1f}%\")\n",
        "\n",
        "print(f\"\\nCleaned data shape: {df.shape}\")\n",
        "print(f\"Assessed value range: ${df['assessed_value'].min():,.2f} to ${df['assessed_value'].max():,.2f}\")\n",
        "\n",
        "mean_value = df['assessed_value'].mean()\n",
        "median_value = df['assessed_value'].median()\n",
        "std_value = df['assessed_value'].std()\n",
        "min_value = df['assessed_value'].min()\n",
        "max_value = df['assessed_value'].max()\n",
        "\n",
        "print(f\"\\nMean Assessed Value: ${mean_value:,.2f}\")\n",
        "print(f\"Median Assessed Value: ${median_value:,.2f}\")\n",
        "print(f\"Standard Deviation: ${std_value:,.2f}\")\n",
        "print(f\"Minimum Value: ${min_value:,.2f}\")\n",
        "print(f\"Maximum Value: ${max_value:,.2f}\")\n",
        "\n",
        "print(df[df['assessed_value'] == 500])\n",
        "\n",
        "# Quartiles and Percentiles\n",
        "quartiles = df['assessed_value'].quantile([0.25, 0.5, 0.75])\n",
        "percentiles = df['assessed_value'].quantile([0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99])\n",
        "\n",
        "print(\"\\nQuartiles:\")\n",
        "print(f\"Q1 (25th percentile): ${quartiles[0.25]:,.2f}\")\n",
        "print(f\"Q2 (50th percentile/Median): ${quartiles[0.5]:,.2f}\")\n",
        "print(f\"Q3 (75th percentile): ${quartiles[0.75]:,.2f}\")\n",
        "\n",
        "print(\"\\nKey Percentiles:\")\n",
        "for pct in [0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]:\n",
        "    print(f\"{int(pct*100)}th percentile: ${percentiles[pct]:,.2f}\")\n",
        "\n",
        "# Select only numeric columns for correlation\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Calculate correlations with assessed_value\n",
        "correlations = df[numeric_cols].corr()['assessed_value'].sort_values(ascending=False)\n",
        "\n",
        "print(\"\\nCorrelation of all numeric factors with Assessed Value:\")\n",
        "print(\"(1.0 = perfect positive correlation, -1.0 = perfect negative correlation, 0 = no correlation)\")\n",
        "print(\"\\n\")\n",
        "for col, corr_value in correlations.items():\n",
        "    if col != 'assessed_value':  # Skip self-correlation\n",
        "        print(f\"{col:30s}: {corr_value:6.3f}\")\n",
        "\n",
        "# Identify strongest correlations (excluding self)\n",
        "correlations_no_self = correlations.drop('assessed_value')\n",
        "strongest_positive = correlations_no_self.nlargest(5)\n",
        "strongest_negative = correlations_no_self.nsmallest(5)\n",
        "\n",
        "print(\"TOP 5 STRONGEST POSITIVE CORRELATIONS:\")\n",
        "for col, corr_value in strongest_positive.items():\n",
        "    print(f\"{col:30s}: {corr_value:6.3f}\")\n",
        "\n",
        "print(\"TOP 5 STRONGEST NEGATIVE CORRELATIONS:\")\n",
        "for col, corr_value in strongest_negative.items():\n",
        "    print(f\"{col:30s}: {corr_value:6.3f}\")\n",
        "\n",
        "# Identify the property type column (may vary in dataset)\n",
        "property_type_col = None\n",
        "for col in df.columns:\n",
        "    col_lower = col.lower()\n",
        "    if 'mill' in col_lower and 'class' in col_lower:\n",
        "        property_type_col = col\n",
        "        break\n",
        "    elif 'property' in col_lower and 'type' in col_lower:\n",
        "        property_type_col = col\n",
        "        break\n",
        "    elif 'assessment' in col_lower and 'class' in col_lower:\n",
        "        property_type_col = col\n",
        "        break\n",
        "\n",
        "if property_type_col:\n",
        "    print(f\"\\nUsing property type column: '{property_type_col}'\")\n",
        "\n",
        "# Group by property type\n",
        "    property_stats = df.groupby(property_type_col)['assessed_value'].agg([\n",
        "      ('count', 'count'),\n",
        "      ('mean', 'mean'),\n",
        "      ('median', 'median'),\n",
        "      ('std', 'std'),\n",
        "      ('min', 'min'),\n",
        "      ('max', 'max')\n",
        "    ]).round(2)\n",
        "\n",
        "    print(f\"\\nStatistics by {property_type_col}:\")\n",
        "    print(property_stats)\n",
        "\n",
        "# Calculate proportion of each property type\n",
        "    print(\"\\nProportion of Each Property Type:\")\n",
        "    proportions = df[property_type_col].value_counts(normalize=True) * 100\n",
        "    print(proportions.round(2))\n",
        "\n",
        "# Identify property types with highest variability (coefficient of variation)\n",
        "    print(\"\\nProperty Types Ranked by Variability (Coefficient of Variation):\")\n",
        "    cv = (property_stats['std'] / property_stats['mean']) * 100\n",
        "    cv_sorted = cv.sort_values(ascending=False)\n",
        "    print(cv_sorted.round(2))\n",
        "else:\n",
        "    print(\"\\nProperty type column not found. Available columns:\")\n",
        "    print(df.columns.tolist())\n",
        "\n",
        "# Identify neighborhood/ward column\n",
        "print(f\"DataFrame shape: {df.shape}\")\n",
        "print(f\"'Neighbourhood' in columns? {'Neighbourhood' in df.columns}\")\n",
        "print(\"\\nAll columns after filtering:\")\n",
        "for i, col in enumerate(df.columns):\n",
        "    print(f\"{i}: '{col}'\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Check for Neighbourhood column\n",
        "if 'Neighbourhood' in df.columns:\n",
        "    geo_col = 'Neighbourhood'\n",
        "elif 'neighbourhood' in df.columns:\n",
        "    geo_col = 'neighbourhood'\n",
        "else:\n",
        "    # Try to find it by searching column names\n",
        "    geo_col = None\n",
        "    for col in df.columns:\n",
        "        if 'neighbour' in col.lower():\n",
        "            geo_col = col\n",
        "            break\n",
        "    if geo_col is None:\n",
        "        print(\"Warning: Neighbourhood column not found!\")\n",
        "        print(\"Available columns:\", df.columns.tolist())\n",
        "        geo_col = 'Neighbourhood'  # Default fallback\n",
        "\n",
        "# Check for Ward column\n",
        "if 'Ward' in df.columns:\n",
        "    ward = 'Ward'\n",
        "elif 'ward' in df.columns:\n",
        "    ward = 'ward'\n",
        "else:\n",
        "    ward = None\n",
        "    for col in df.columns:\n",
        "        if 'ward' in col.lower():\n",
        "            ward = col\n",
        "            break\n",
        "    if ward is None:\n",
        "        print(\"Warning: Ward column not found!\")\n",
        "        ward = 'Ward'  # Default fallback\n",
        "\n",
        "print(f\"'Neighbourhood' in columns? {geo_col in df.columns}\")\n",
        "print(f\"'Ward' in columns? {ward in df.columns}\")\n",
        "\n",
        "# Calculate average assessed values by neighborhood\n",
        "if geo_col in df.columns:\n",
        "    neighbourhood_stats = df.groupby(geo_col)['assessed_value'].agg([\n",
        "        ('count', 'count'),\n",
        "        ('mean', 'mean'),\n",
        "        ('median', 'median')\n",
        "    ]).round(2)\n",
        "\n",
        "    neighbourhood_stats = neighbourhood_stats.sort_values('mean', ascending=False)\n",
        "\n",
        "    print(f\"\\nTop 10 Most Expensive Neighbourhoods (by mean value):\")\n",
        "    print(neighbourhood_stats.head(10))\n",
        "\n",
        "    print(f\"\\nTop 10 Least Expensive Neighbourhoods (by mean value):\")\n",
        "    print(neighbourhood_stats.tail(10))\n",
        "else:\n",
        "    print(f\"\\nSkipping neighbourhood analysis - column '{geo_col}' not found\")\n",
        "\n",
        "# Calculate average assessed values by ward\n",
        "if ward in df.columns:\n",
        "    ward_stats = df.groupby(ward)['assessed_value'].agg([\n",
        "        ('count', 'count'),\n",
        "        ('mean', 'mean'),\n",
        "        ('median', 'median')\n",
        "    ]).round(2)\n",
        "\n",
        "    ward_stats = ward_stats.sort_values('mean', ascending=False)\n",
        "\n",
        "    print(f\"\\nMost Expensive Ward (by mean value):\")\n",
        "    print(ward_stats.head(1))\n",
        "\n",
        "    print(f\"\\nLeast Expensive Ward (by mean value):\")\n",
        "    print(ward_stats.tail(1))\n",
        "else:\n",
        "    print(f\"\\nSkipping ward analysis - column '{ward}' not found\")\n",
        "\n",
        "# Create value brackets\n",
        "brackets = [0, 200000, 400000, 600000, 800000, 1000000, float('inf')]\n",
        "labels = ['<$200K', '$200K-400K', '$400K-600K', '$600K-800K', '$800K-1M', '>$1M']\n",
        "\n",
        "df['value_bracket'] = pd.cut(df['assessed_value'], bins=brackets, labels=labels)\n",
        "\n",
        "# Count properties in each bracket\n",
        "bracket_counts = df['value_bracket'].value_counts().sort_index()\n",
        "print(\"\\nProperties by Value Bracket:\")\n",
        "print(bracket_counts)\n",
        "\n",
        "# Analyze characteristics by bracket\n",
        "print(\"\\nCharacteristics by Value Bracket:\")\n",
        "bracket_analysis = df.groupby('value_bracket').agg({\n",
        "    'assessed_value': ['count', 'mean', 'median']\n",
        "})\n",
        "print(bracket_analysis)\n",
        "\n",
        "# VISUALIZATION 1: Interactive Histogram - Distribution of Assessed Values\n",
        "hist, edges = np.histogram(df['assessed_value'], bins=50)\n",
        "\n",
        "p1 = figure(title='Distribution of Property Assessed Values',\n",
        "           x_axis_label='Assessed Value ($)',\n",
        "           y_axis_label='Frequency',\n",
        "           width=800, height=400,\n",
        "           x_range=(0, 2500000))\n",
        "\n",
        "p1.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:],\n",
        "        fill_color='navy', line_color='white', alpha=0.7)\n",
        "\n",
        "from bokeh.models import NumeralTickFormatter\n",
        "p1.xaxis.ticker = [0, 250000, 500000, 750000, 1000000, 1250000, 1500000, 1750000, 2000000, 2250000, 2500000]\n",
        "p1.xaxis.formatter = NumeralTickFormatter(format=\"$0,0\")\n",
        "p1.yaxis.ticker = [0, 50000, 100000, 150000, 200000, 250000, 300000]\n",
        "p1.yaxis.formatter = NumeralTickFormatter(format=\"0,0\")\n",
        "\n",
        "p1.add_tools(HoverTool(tooltips=[('Range', '@left{0,0} - @right{0,0}'),\n",
        "                                  ('Count', '@top')]))\n",
        "\n",
        "show(p1)\n",
        "\n",
        "# VISUALIZATION 2: Interactive Bar Chart - Average Values by Neighbourhood\n",
        "if geo_col:\n",
        "    # Get top 20 neighbourhoods by mean value\n",
        "    top_neighbourhoods = neighbourhood_stats.head(20).reset_index()\n",
        "\n",
        "    source = ColumnDataSource(top_neighbourhoods)\n",
        "\n",
        "    p2 = figure(x_range=top_neighbourhoods[geo_col].tolist(),\n",
        "               title='Top 20 Neighbourhoods by Average Assessed Value',\n",
        "               x_axis_label='Neighbourhood',\n",
        "               y_axis_label='Average Assessed Value ($)',\n",
        "               width=1000, height=500)\n",
        "\n",
        "    p2.vbar(x=geo_col, top='mean', width=0.8, source=source,\n",
        "            line_color='white', fill_color='green', alpha=0.8)\n",
        "\n",
        "    p2.xaxis.major_label_orientation = 45\n",
        "\n",
        "    from bokeh.models import NumeralTickFormatter\n",
        "    p2.yaxis.ticker = [0, 5000000, 10000000, 15000000, 20000000]\n",
        "    p2.yaxis.formatter = NumeralTickFormatter(format=\"$0,0\")\n",
        "\n",
        "    p2.add_tools(HoverTool(tooltips=[('Neighbourhood', f'@{geo_col}'),\n",
        "                                      ('Avg Value', '@mean{$0,0}'),\n",
        "                                      ('Count', '@count')]))\n",
        "\n",
        "    show(p2)\n",
        "\n",
        "# VISUALIZATION 3: Interactive Bar Chart - Average Values by Ward\n",
        "if ward and ward in df.columns:\n",
        "    # Get top 5 wards by mean value\n",
        "    top_wards = ward_stats.head(5).reset_index()\n",
        "\n",
        "    source = ColumnDataSource(top_wards)\n",
        "\n",
        "    p3 = figure(x_range=top_wards[ward].tolist(),\n",
        "               title='Top 5 Wards by Average Assessed Value',\n",
        "               x_axis_label='Ward',\n",
        "               y_axis_label='Average Assessed Value ($)',\n",
        "               width=1000, height=500)\n",
        "\n",
        "    p3.vbar(x=ward, top='mean', width=0.8, source=source,\n",
        "            line_color='white', fill_color='green', alpha=0.8)\n",
        "\n",
        "    p3.xaxis.major_label_orientation = 45\n",
        "\n",
        "    from bokeh.models import NumeralTickFormatter\n",
        "    p3.yaxis.ticker = [0, 100000, 200000, 300000, 400000, 500000]\n",
        "    p3.yaxis.formatter = NumeralTickFormatter(format=\"$0,0\")\n",
        "\n",
        "    p3.add_tools(HoverTool(tooltips=[('Ward', f'@{ward}'),\n",
        "                                      ('Avg Value', '@mean{$0,0}'),\n",
        "                                      ('Count', '@count')]))\n",
        "\n",
        "    show(p3)\n",
        "\n",
        "# VISUALIZATION 4: Interactive Heatmap - Geographic Distribution\n",
        "print(\"Has Latitude?\", 'Latitude' in df.columns)\n",
        "print(\"Has Longitude?\", 'Longitude' in df.columns)\n",
        "from bokeh.models import NumeralTickFormatter\n",
        "\n",
        "if 'Latitude' in df.columns and 'Longitude' in df.columns:\n",
        "    # Remove missing coordinates\n",
        "    df_map = df.dropna(subset=['Latitude', 'Longitude'])\n",
        "\n",
        "    # Create color mapper for assessed values\n",
        "    mapper = linear_cmap(field_name='assessed_value',\n",
        "                        palette=Viridis256,\n",
        "                        low=df_map['assessed_value'].quantile(0.05),\n",
        "                        high=df_map['assessed_value'].quantile(0.95))\n",
        "\n",
        "    source = ColumnDataSource(df_map)\n",
        "\n",
        "    p4 = figure(title='Geographic Distribution of Property Values',\n",
        "               x_axis_label='Longitude',\n",
        "               y_axis_label='Latitude',\n",
        "               width=900, height=700,\n",
        "               tools='pan,wheel_zoom,box_zoom,reset')\n",
        "\n",
        "    p4.circle('Longitude', 'Latitude', source=source,\n",
        "             size=3, alpha=0.6, color=mapper)\n",
        "\n",
        "    color_bar = ColorBar(color_mapper=mapper['transform'],\n",
        "                        width=8, location=(0,0),\n",
        "                        title=\"Assessed Value ($)\",\n",
        "                        formatter=NumeralTickFormatter(format=\"$0,0\"))\n",
        "    p4.add_layout(color_bar, 'right')\n",
        "\n",
        "    p4.add_tools(HoverTool(tooltips=[('Location', '@latitude{0.0000}, @longitude{0.0000}'),\n",
        "                                      ('Value', '@assessed_value{$0,0}')]))\n",
        "\n",
        "    show(p4)\n",
        "\n",
        "#VISUALIZATION 5: Correlation\n",
        "# Get top 10 correlations (positive and negative)\n",
        "top_10_corr = pd.concat([\n",
        "    strongest_positive.head(5),\n",
        "    strongest_negative.head(5)\n",
        "]).sort_values().drop_duplicates()\n",
        "\n",
        "# Create bar chart\n",
        "from bokeh.models import Title\n",
        "\n",
        "# Color bars based on positive/negative\n",
        "colors = ['red' if x < 0 else 'green' for x in top_10_corr.values]\n",
        "\n",
        "source_corr = ColumnDataSource(data={\n",
        "    'factors': top_10_corr.index.tolist(),\n",
        "    'correlation': top_10_corr.values.tolist(),\n",
        "    'colors': colors\n",
        "})\n",
        "\n",
        "p_corr = figure(\n",
        "    y_range=top_10_corr.index.tolist(),\n",
        "    title='Top 10 Factors Correlated with Assessed Value',\n",
        "    x_axis_label='Correlation Coefficient',\n",
        "    y_axis_label='Factor',\n",
        "    width=900,\n",
        "    height=500\n",
        ")\n",
        "\n",
        "p_corr.hbar(\n",
        "    y='factors',\n",
        "    right='correlation',\n",
        "    height=0.7,\n",
        "    source=source_corr,\n",
        "    color='colors',\n",
        "    alpha=0.8\n",
        ")\n",
        "\n",
        "# Add a vertical line at x=0\n",
        "from bokeh.models import Span\n",
        "vline = Span(location=0, dimension='height', line_color='black', line_width=2)\n",
        "p_corr.add_layout(vline)\n",
        "\n",
        "p_corr.add_tools(HoverTool(tooltips=[\n",
        "    ('Factor', '@factors'),\n",
        "    ('Correlation', '@correlation{0.000}')\n",
        "]))\n",
        "\n",
        "subtitle = Title(text=\"Green = positive correlation, Red = negative correlation\",\n",
        "                text_font_size=\"10pt\", text_font_style=\"italic\")\n",
        "p_corr.add_layout(subtitle, 'above')\n",
        "\n",
        "show(p_corr)\n",
        "\n",
        "# VISUALIZATION 6: Interactive Map - Property Values\n",
        "import json\n",
        "\n",
        "# Prepare data for map (sample for performance if dataset is huge)\n",
        "if 'Latitude' in df.columns and 'Longitude' in df.columns:\n",
        "    df_map = df.dropna(subset=['Latitude', 'Longitude']).copy()\n",
        "\n",
        "    # If dataset is very large, sample it for better map performance\n",
        "    if len(df_map) > 10000:\n",
        "        print(f\"Dataset has {len(df_map)} properties. Sampling 10,000 for map performance...\")\n",
        "        df_map = df_map.sample(n=10000, random_state=42)\n",
        "\n",
        "    # Create value brackets for color coding\n",
        "    df_map['value_category'] = pd.cut(\n",
        "        df_map['assessed_value'],\n",
        "        bins=[0, 200000, 400000, 600000, 800000, float('inf')],\n",
        "        labels=['<$200K', '$200K-400K', '$400K-600K', '$600K-800K', '>$800K']\n",
        "    )\n",
        "\n",
        "    # Map categories to numeric values for color mapping\n",
        "    category_to_num = {\n",
        "        '<$200K': 1,\n",
        "        '$200K-400K': 2,\n",
        "        '$400K-600K': 3,\n",
        "        '$600K-800K': 4,\n",
        "        '>$800K': 5\n",
        "    }\n",
        "    df_map['color_value'] = df_map['value_category'].map(category_to_num)\n",
        "\n",
        "    # Create color mapper\n",
        "    from bokeh.models import CategoricalColorMapper\n",
        "    color_mapper = CategoricalColorMapper(\n",
        "        factors=['<$200K', '$200K-400K', '$400K-600K', '$600K-800K', '>$800K'],\n",
        "        palette=['blue', 'green', 'orange', 'red', 'darkred']\n",
        "    )\n",
        "\n",
        "    # Prepare tooltip data\n",
        "    df_map['value_formatted'] = df_map['assessed_value'].apply(lambda x: f\"${x:,.0f}\")\n",
        "\n",
        "    # Create data source\n",
        "    source = ColumnDataSource(df_map)\n",
        "\n",
        "    # Create the map figure\n",
        "    p_map = figure(\n",
        "        title='Interactive Map: Edmonton Property Values',\n",
        "        x_axis_label='Longitude',\n",
        "        y_axis_label='Latitude',\n",
        "        width=1000,\n",
        "        height=700,\n",
        "        tools='pan,wheel_zoom,box_zoom,reset,save'\n",
        "    )\n",
        "\n",
        "    # Add property points\n",
        "    p_map.scatter(\n",
        "        'Longitude',\n",
        "        'Latitude',\n",
        "        source=source,\n",
        "        size=6,\n",
        "        alpha=0.6,\n",
        "        color={'field': 'value_category', 'transform': color_mapper},\n",
        "        marker='circle',\n",
        "        line_color='white',\n",
        "        line_width=0.5\n",
        "    )\n",
        "\n",
        "    # Create hover tooltips\n",
        "    tooltips = [\n",
        "        ('Value', '@value_formatted'),\n",
        "        ('Category', '@value_category'),\n",
        "        ('Location', '(@latitude{0.0000}, @longitude{0.0000})')\n",
        "    ]\n",
        "\n",
        "    # Add neighborhood and property type to tooltips if available\n",
        "    if geo_col and geo_col in df_map.columns:\n",
        "        tooltips.append(('Neighborhood', f'@{geo_col}'))\n",
        "\n",
        "    if property_type_col and property_type_col in df_map.columns:\n",
        "        tooltips.append(('Property Type', f'@{property_type_col}'))\n",
        "\n",
        "    hover = HoverTool(tooltips=tooltips)\n",
        "    p_map.add_tools(hover)\n",
        "\n",
        "    # Add title with instructions\n",
        "    from bokeh.models import Title\n",
        "    subtitle = Title(text=\"Use mouse to pan and scroll to zoom. Hover over points for details.\",\n",
        "                    text_font_size=\"10pt\", text_font_style=\"italic\")\n",
        "    p_map.add_layout(subtitle, 'above')\n",
        "\n",
        "    # Save map to HTML file\n",
        "    from bokeh.plotting import output_file, save\n",
        "    output_file('edmonton_map.html')\n",
        "    save(p_map)\n",
        "    print(\"âœ“ Map saved to edmonton_map.html\")\n",
        "\n",
        "    # Prepare data for JSON export (sample for reasonable file size)\n",
        "    df_json = df_map.copy()\n",
        "\n",
        "    # Select relevant columns for JSON\n",
        "    json_columns = ['assessed_value', 'latitude', 'longitude']\n",
        "    if geo_col and geo_col in df_map.columns:\n",
        "        json_columns.append(geo_col)\n",
        "    if property_type_col and property_type_col in df_map.columns:\n",
        "        json_columns.append(property_type_col)\n",
        "\n",
        "    # Create JSON-friendly data structure\n",
        "    properties_data = []\n",
        "    for idx, row in df_json.iterrows():\n",
        "        property_dict = {\n",
        "            'assessed_value': float(row['assessed_value']),\n",
        "            'latitude': float(row['Latitude']),\n",
        "            'longitude': float(row['Longitude']),\n",
        "            'value_category': str(row['value_category'])\n",
        "        }\n",
        "\n",
        "        if geo_col and geo_col in row:\n",
        "            property_dict['neighbourhood'] = str(row[geo_col])\n",
        "\n",
        "        if property_type_col and property_type_col in row:\n",
        "            property_dict['property_type'] = str(row[property_type_col])\n",
        "\n",
        "        properties_data.append(property_dict)\n",
        "\n",
        "    # Create complete JSON structure with metadata\n",
        "    json_output = {\n",
        "        'metadata': {\n",
        "            'total_properties': len(df),\n",
        "            'properties_in_export': len(properties_data),\n",
        "            'mean_value': float(mean_value),\n",
        "            'median_value': float(median_value),\n",
        "            'min_value': float(min_value),\n",
        "            'max_value': float(max_value),\n",
        "            'city': 'Edmonton',\n",
        "            'data_year': 2024\n",
        "        },\n",
        "        'properties': properties_data\n",
        "    }\n",
        "\n",
        "    # Save to JSON file\n",
        "    json_filename = 'edmonton_properties.json'\n",
        "    with open(json_filename, 'w') as f:\n",
        "        json.dump(json_output, f, indent=2)\n",
        "\n",
        "    # Display sample of JSON structure\n",
        "    print(\"\\nSample JSON structure:\")\n",
        "    print(json.dumps(json_output['properties'][0], indent=2))\n",
        "\n",
        "# Create summary dictionary for easy reference\n",
        "summary = {\n",
        "    'Total Properties': len(df),\n",
        "    'Mean Value': f\"${mean_value:,.2f}\",\n",
        "    'Median Value': f\"${median_value:,.2f}\",\n",
        "    'Std Deviation': f\"${std_value:,.2f}\",\n",
        "    'Min Value': f\"${min_value:,.2f}\",\n",
        "    'Max Value': f\"${max_value:,.2f}\"\n",
        "}\n",
        "\n",
        "print(\"\\nSummary Statistics:\")\n",
        "for key, value in summary.items():\n",
        "    print(f\"{key}: {value}\")"
      ]
    }
  ]
}